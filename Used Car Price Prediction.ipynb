{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9ec26ae-75ce-4d9d-92ef-e73eece6943c",
   "metadata": {},
   "source": [
    "# Used Car Price Prediction\n",
    "\n",
    "**Problem Statement:**  \n",
    "Predict the **log-transformed selling price** of **used cars** using relevant **vehicle and customer features** to improve pricing accuracy and transparency.\n",
    "\n",
    "---\n",
    "\n",
    "**By:** *Mowlick Armstrong*\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 1: Importing Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3357cdcd-559f-4261-a689-e29565f82cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "df = pd.read_csv('used_car_dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63947b29-8593-4ead-8cfc-4f8ecf0ee4e7",
   "metadata": {},
   "source": [
    "#### Step 2: Intial Data Inspection and Understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5725f284-0d9d-4745-9f32-680204712fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first 5 rows of the DataFrame\n",
    "print(\"First 5 rows of the dataset:\")\n",
    "print(df.head())\n",
    "\n",
    "# Display basic information about the DataFrame (columns, non-null counts, data types)\n",
    "print(\"\\nDataFrame Info:\")\n",
    "df.info()\n",
    "\n",
    "# Display descriptive statistics for numerical columns\n",
    "print(\"\\nDescriptive Statistics for Numerical Columns:\")\n",
    "print(df.describe())\n",
    "\n",
    "# Display descriptive statistics for object (categorical) columns\n",
    "print(\"\\nDescriptive Statistics for Categorical Columns:\")\n",
    "print(df.describe(include='object'))\n",
    "\n",
    "# Check for the number of unique values in each column\n",
    "print(\"\\nNumber of Unique Values per Column:\")\n",
    "print(df.nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db7514b-4757-48a6-a568-0282810f4ef0",
   "metadata": {},
   "source": [
    "#### Step 3: Step 3: Handling Missing Values (Preliminary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e280e9-b23b-457d-a0b8-ea86bba47977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in each column\n",
    "print(\"\\nMissing Values Count per Column:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Calculate the percentage of missing values for each column\n",
    "print(\"\\nPercentage of Missing Values per Column:\")\n",
    "print((df.isnull().sum() / len(df)) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f2ec89-34b3-4a72-87a8-7e7632174c98",
   "metadata": {},
   "source": [
    "#### Step 4: Univariate Analysis of the Target Variable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba90c666-5b72-4770-bd81-290f125da72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = df[(df['price'] >= 1000) & (df['price'] <= 1e7)].copy()  \n",
    "\n",
    "# Log-transform the filtered price safely\n",
    "filtered_df.loc[:, 'log_price'] = np.log1p(filtered_df['price'])\n",
    "\n",
    "# Plot the log-transformed distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(filtered_df['log_price'], bins=50, kde=True)\n",
    "plt.title('Log-Transformed Distribution of Car Prices')\n",
    "plt.xlabel('Log(Price + 1)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Show statistics\n",
    "print(\"\\nDescriptive Statistics for original 'price':\")\n",
    "print(df['price'].describe())\n",
    "\n",
    "print(\"\\nDescriptive Statistics for filtered 'price':\")\n",
    "print(filtered_df['price'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d92418a-39dc-4e96-b3f8-28e8cab1075b",
   "metadata": {},
   "source": [
    "#### Step 5: Handling Outliers in price and Other Numerical Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780bcab1-8ade-4694-9f59-013c593a4dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize outliers for 'price' using a horizontal box plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x=df['price']) # Changed 'y' to 'x'\n",
    "plt.title('Box Plot of Car Prices')\n",
    "plt.xlabel('Price') # Changed 'ylabel' to 'xlabel'\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Visualize outliers for 'odometer' using a horizontal box plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x=df['odometer']) # Changed 'y' to 'x'\n",
    "plt.title('Box Plot of Odometer Readings')\n",
    "plt.xlabel('Odometer') # Changed 'ylabel' to 'xlabel'\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Visualize outliers for 'year' using a horizontal box plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x=df['year']) # Changed 'y' to 'x'\n",
    "plt.title('Box Plot of Car Year')\n",
    "plt.xlabel('Year') # Changed 'ylabel' to 'xlabel'\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# ---\n",
    "# Optional: Identify specific values that are potential outliers (e.g., using IQR method for 'price')\n",
    "# This part remains the same as it's not dependent on plot orientation\n",
    "Q1_price = df['price'].quantile(0.25)\n",
    "Q3_price = df['price'].quantile(0.75)\n",
    "IQR_price = Q3_price - Q1_price\n",
    "lower_bound_price = Q1_price - 1.5 * IQR_price\n",
    "upper_bound_price = Q3_price + 1.5 * IQR_price\n",
    "\n",
    "print(f\"\\nPrice Outlier Bounds (IQR Method):\")\n",
    "print(f\"Lower Bound: {lower_bound_price}\")\n",
    "print(f\"Upper Bound: {upper_bound_price}\")\n",
    "print(f\"Number of prices below lower bound: {df[df['price'] < lower_bound_price].shape[0]}\")\n",
    "print(f\"Number of prices above upper bound: {df[df['price'] > upper_bound_price].shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f6f5c0-3eb6-42c5-ba8c-be7d0df21eaf",
   "metadata": {},
   "source": [
    "#### Step 6: Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453db53e-dd14-476a-a508-7a8398b974ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create 'age' from 'year'\n",
    "# Assuming the current year is 2025 for calculation purposes.\n",
    "# You might want to get the actual current year or consider the posting_date if available.\n",
    "df['age'] = 2025 - df['year']\n",
    "print(\"\\n'age' column created. First 5 values:\")\n",
    "print(df['age'].head())\n",
    "\n",
    "# 2. Extract 'region_category' from 'region' (e.g., city/state or major metro area)\n",
    "df['is_sf_bay_area'] = df['region'].apply(lambda x: 1 if 'sf bay area' in str(x).lower() else 0)\n",
    "print(\"\\n'is_sf_bay_area' column created. First 5 values:\")\n",
    "print(df['is_sf_bay_area'].head())\n",
    "\n",
    "\n",
    "# 3. Simplify 'condition' into numerical scale \n",
    "condition_mapping = {\n",
    "    'new': 5,\n",
    "    'excellent': 4,\n",
    "    'good': 3,\n",
    "    'fair': 2,\n",
    "    'salvage': 1,\n",
    "    'lien': 0 # Added 'lien' based on common Craigslist conditions\n",
    "}\n",
    "df['condition_numeric'] = df['condition'].map(condition_mapping)\n",
    "print(\"\\n'condition_numeric' column created. First 5 values:\")\n",
    "print(df['condition_numeric'].head())\n",
    "print(\"Value counts for 'condition_numeric':\")\n",
    "print(df['condition_numeric'].value_counts(dropna=False)) # Check how many NaNs might have resulted\n",
    "\n",
    "# 4. Binary feature for 'VIN' presence (if VIN is a strong indicator of legitimacy/detailed history)\n",
    "df['has_vin'] = df['VIN'].apply(lambda x: 1 if pd.notna(x) else 0)\n",
    "print(\"\\n'has_vin' column created. First 5 values:\")\n",
    "print(df['has_vin'].head())\n",
    "\n",
    "# Display the columns and their types after feature engineering\n",
    "print(\"\\nDataFrame Info after Feature Engineering:\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3870e2f-b379-4cf4-a529-4f33c82fea90",
   "metadata": {},
   "source": [
    "#### Step 7: Bivariate Analysis (Numerical Features vs. Price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782a0ed6-c49f-4f95-9030-a94e163d2f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re # Import regular expression module\n",
    "\n",
    "# Function to clean and convert 'cylinders'\n",
    "def clean_cylinders(value):\n",
    "    if pd.isna(value):\n",
    "        return np.nan # Keep NaN if missing\n",
    "    try:\n",
    "        # Try to convert directly to float if it's already a number\n",
    "        return float(value)\n",
    "    except ValueError:\n",
    "        # If it's a string like '8 cylinders', extract the number\n",
    "        match = re.search(r'(\\d+)', str(value))\n",
    "        if match:\n",
    "            return float(match.group(1))\n",
    "        else:\n",
    "            return np.nan # If no number found, treat as NaN or another placeholder\n",
    "            # You might also consider returning a mode/median based on EDA for imputation\n",
    "            # For now, let's keep it NaN to see how many fail conversion\n",
    "            # print(f\"Warning: Could not convert '{value}' in 'cylinders'\") # Uncomment to see problematic values\n",
    "            # return np.nan # Return NaN if unable to parse\n",
    "            # Alternatively, you could return a common value like df['cylinders'].mode()[0] if you want to impute immediately.\n",
    "\n",
    "# Apply the cleaning function to the 'cylinders' column\n",
    "df['cylinders_cleaned'] = df['cylinders'].apply(clean_cylinders)\n",
    "\n",
    "# Check the new 'cylinders_cleaned' column info and value counts\n",
    "print(\"\\nInfo for 'cylinders_cleaned' after cleaning:\")\n",
    "df['cylinders_cleaned'].info()\n",
    "print(\"\\nValue Counts for 'cylinders_cleaned':\")\n",
    "print(df['cylinders_cleaned'].value_counts(dropna=False))\n",
    "\n",
    "# Drop the original 'cylinders' column if you wish, or keep it for comparison\n",
    "# df = df.drop('cylinders', axis=1) # You can uncomment this line if you want to remove the original column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bded63-2454-433c-b414-e825843cf560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot: Price vs. Year\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='year', y='price', data=df, alpha=0.6)\n",
    "plt.title('Price vs. Year')\n",
    "plt.xlabel('Year of Manufacture')\n",
    "plt.ylabel('Price')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Scatter plot: Price vs. Age (newly engineered feature)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='age', y='price', data=df, alpha=0.6)\n",
    "plt.title('Price vs. Age of Car')\n",
    "plt.xlabel('Age (Years)')\n",
    "plt.ylabel('Price')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Scatter plot: Price vs. Odometer\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='odometer', y='price', data=df, alpha=0.6)\n",
    "plt.title('Price vs. Odometer Reading')\n",
    "plt.xlabel('Odometer Reading')\n",
    "plt.ylabel('Price')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Scatter plot: Price vs. Cylinders (using the CLEANED column)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='cylinders_cleaned', y='price', data=df, alpha=0.6)\n",
    "plt.title('Price vs. Number of Cylinders')\n",
    "plt.xlabel('Number of Cylinders')\n",
    "plt.ylabel('Price')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Calculate Pearson correlation coefficients for key numerical features with 'price'\n",
    "# IMPORTANT: Use 'cylinders_cleaned' here!\n",
    "numerical_cols_for_corr = ['price', 'year', 'age', 'odometer', 'cylinders_cleaned', 'condition_numeric']\n",
    "print(\"\\nCorrelation Matrix (selected numerical features with price):\")\n",
    "print(df[numerical_cols_for_corr].corr()['price'].sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f4513d-c34e-4664-ac46-d08ca259a700",
   "metadata": {},
   "source": [
    "#### Step 8: Bivariate Analysis (Categorical Features vs. Price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1cad4d-b3e5-4c30-8360-398fe69c9fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create box plots for categorical features vs. price\n",
    "def plot_categorical_vs_price(df, column):\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    sns.boxplot(x=column, y='price', data=df)\n",
    "    plt.title(f'Price Distribution by {column.replace(\"_\", \" \").title()}')\n",
    "    plt.xlabel(column.replace(\"_\", \" \").title())\n",
    "    plt.ylabel('Price')\n",
    "    plt.xticks(rotation=45, ha='right') # Rotate labels for better readability\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Analyze 'manufacturer' vs. price\n",
    "print(\"\\nAnalyzing 'manufacturer' vs. price:\")\n",
    "top_manufacturers = df['manufacturer'].value_counts().nlargest(20).index\n",
    "plot_categorical_vs_price(df[df['manufacturer'].isin(top_manufacturers)], 'manufacturer')\n",
    "\n",
    "\n",
    "# Analyze 'condition' vs. price (using the cleaned numerical 'condition_numeric')\n",
    "print(\"\\nAnalyzing 'condition' vs. price:\")\n",
    "plot_categorical_vs_price(df, 'condition') # Use original 'condition' for better labels\n",
    "\n",
    "\n",
    "# Analyze 'fuel' vs. price\n",
    "print(\"\\nAnalyzing 'fuel' vs. price:\")\n",
    "plot_categorical_vs_price(df, 'fuel')\n",
    "\n",
    "\n",
    "# Analyze 'transmission' vs. price\n",
    "print(\"\\nAnalyzing 'transmission' vs. price:\")\n",
    "plot_categorical_vs_price(df, 'transmission')\n",
    "\n",
    "\n",
    "# Analyze 'type' vs. price\n",
    "print(\"\\nAnalyzing 'type' vs. price:\")\n",
    "plot_categorical_vs_price(df, 'type')\n",
    "\n",
    "\n",
    "# Analyze 'drive' vs. price\n",
    "print(\"\\nAnalyzing 'drive' vs. price:\")\n",
    "plot_categorical_vs_price(df, 'drive')\n",
    "\n",
    "# Analyze 'paint_color' vs. price\n",
    "print(\"\\nAnalyzing 'paint_color' vs. price:\")\n",
    "top_colors = df['paint_color'].value_counts().nlargest(15).index\n",
    "plot_categorical_vs_price(df[df['paint_color'].isin(top_colors)], 'paint_color')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04901b0a-adc5-49f4-8973-2f0035289794",
   "metadata": {},
   "source": [
    "#### Step 9: Correlation Heatmap and Feature Selection Considerations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14d5352-a495-4cf3-bb48-fbdf747fc3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only numerical columns for the correlation heatmap\n",
    "numerical_features = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# Exclude 'id', 'lat', 'long' if they are present and not truly numerical features for correlation\n",
    "features_to_exclude = ['id', 'lat', 'long', 'url', 'region_url', 'image_url', 'VIN', 'county'] # Add other non-numeric/identifier columns if needed\n",
    "numerical_features = [col for col in numerical_features if col not in features_to_exclude]\n",
    "\n",
    "# Create the correlation matrix\n",
    "correlation_matrix = df[numerical_features].corr()\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=.5)\n",
    "plt.title('Correlation Heatmap of Numerical Features')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTop correlations with Price:\")\n",
    "print(correlation_matrix['price'].sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1a66c1-ec73-4bd1-9a0d-eb39beed0c48",
   "metadata": {},
   "source": [
    "#### Step 10: Handling Remaining Missing Values and Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfc3056-0f2a-4b45-bbb8-a0481b4d4a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- REVISED SECTION FOR STEP 10.1: Handling Missing Values ---\n",
    "\n",
    "print(\"Missing values before imputation:\")\n",
    "print(df.isnull().sum()[df.isnull().sum() > 0]) # Show only columns with NaNs\n",
    "\n",
    "# --- Numerical Imputation ---\n",
    "# Example: Imputing 'odometer' and 'cylinders_cleaned' with their medians/modes\n",
    "if 'odometer' in df.columns and df['odometer'].isnull().any():\n",
    "    median_odometer = df['odometer'].median()\n",
    "    df['odometer'] = df['odometer'].fillna(median_odometer)\n",
    "    print(f\"\\n'odometer' missing values filled with median: {median_odometer}\")\n",
    "\n",
    "if 'cylinders_cleaned' in df.columns and df['cylinders_cleaned'].isnull().any():\n",
    "    mode_cylinders = df['cylinders_cleaned'].mode()[0]\n",
    "    df['cylinders_cleaned'] = df['cylinders_cleaned'].fillna(mode_cylinders)\n",
    "    print(f\"\\n'cylinders_cleaned' missing values filled with mode: {mode_cylinders}\")\n",
    "\n",
    "if 'condition_numeric' in df.columns and df['condition_numeric'].isnull().any():\n",
    "    mode_condition_numeric = df['condition_numeric'].mode()[0]\n",
    "    df['condition_numeric'] = df['condition_numeric'].fillna(mode_condition_numeric)\n",
    "    print(f\"\\n'condition_numeric' missing values filled with mode: {mode_condition_numeric}\")\n",
    "\n",
    "# ADDED THIS BLOCK FOR 'age' IMPUTATION\n",
    "if 'age' in df.columns and df['age'].isnull().any():\n",
    "    median_age = df['age'].median()\n",
    "    df['age'] = df['age'].fillna(median_age)\n",
    "    print(f\"\\n'age' missing values filled with median: {median_age}\")\n",
    "\n",
    "\n",
    "# --- Categorical Imputation / Dropping (no changes here) ---\n",
    "if 'paint_color' in df.columns and df['paint_color'].isnull().any():\n",
    "    mode_paint_color = df['paint_color'].mode()[0]\n",
    "    df['paint_color'] = df['paint_color'].fillna(mode_paint_color)\n",
    "    print(f\"\\n'paint_color' missing values filled with mode: {mode_paint_color}\")\n",
    "\n",
    "# Example: Dropping columns with a very high percentage of missing values (no change here)\n",
    "cols_to_drop_if_too_many_nans = []\n",
    "\n",
    "if 'VIN' in df.columns:\n",
    "    vin_nan_percentage = (df['VIN'].isnull().sum() / len(df)) * 100\n",
    "    if vin_nan_percentage > 70:\n",
    "        cols_to_drop_if_too_many_nans.append('VIN')\n",
    "        print(f\"\\n'VIN' has {vin_nan_percentage:.2f}% missing values, marked for dropping.\")\n",
    "\n",
    "if 'county' in df.columns:\n",
    "    county_nan_percentage = (df['county'].isnull().sum() / len(df)) * 100\n",
    "    if county_nan_percentage > 70:\n",
    "        cols_to_drop_if_too_many_nans.append('county')\n",
    "        print(f\"\\n'county' has {county_nan_percentage:.2f}% missing values, marked for dropping.\")\n",
    "\n",
    "if cols_to_drop_if_too_many_nans:\n",
    "    df.drop(columns=cols_to_drop_if_too_many_nans, inplace=True)\n",
    "    print(f\"\\nDropped columns: {cols_to_drop_if_too_many_nans}\")\n",
    "\n",
    "# Verify remaining missing values\n",
    "print(\"\\nMissing values after imputation/dropping:\")\n",
    "print(df.isnull().sum()[df.isnull().sum() > 0])\n",
    "\n",
    "# --- End of REVISED SECTION FOR STEP 10.1 ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873543ea-75f2-4056-804b-ad95a9379756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log Transform highly skewed numerical features: 'price' and 'odometer'\n",
    "# We'll create new columns for the transformed versions to preserve original data\n",
    "# Add 1 to avoid log(0) issues, although prices/odometers are unlikely to be 0\n",
    "df['price_log'] = np.log1p(df['price']) # log1p is log(1+x)\n",
    "df['odometer_log'] = np.log1p(df['odometer'])\n",
    "\n",
    "print(\"\\nOriginal vs. Log Transformed Price (first 5 values):\")\n",
    "print(df[['price', 'price_log']].head())\n",
    "\n",
    "print(\"\\nOriginal vs. Log Transformed Odometer (first 5 values):\")\n",
    "print(df[['odometer', 'odometer_log']].head())\n",
    "\n",
    "# Plot distribution of transformed 'price_log'\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df['price_log'], bins=50, kde=True)\n",
    "plt.title('Distribution of Log-Transformed Car Prices')\n",
    "plt.xlabel('Log(Price)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178bb542-76ab-4939-afb8-6d57d9981132",
   "metadata": {},
   "source": [
    "#### Step 11: Encoding Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84797a3e-6b71-41fd-86c3-01df959793e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify categorical columns to be one-hot encoded\n",
    "# Exclude columns that are identifiers or already handled (like region_url, VIN, image_url, url, description, region, county)\n",
    "# Also exclude 'condition' since we already have 'condition_numeric'\n",
    "# 'model' can have very high cardinality, so we'll decide on that later or simplify it.\n",
    "# For now, let's focus on: manufacturer, fuel, transmission, drive, type, paint_color, title_status, is_sf_bay_area\n",
    "\n",
    "categorical_cols_to_encode = [\n",
    "    'manufacturer',\n",
    "    'fuel',\n",
    "    'transmission',\n",
    "    'drive',\n",
    "    'type',\n",
    "    'paint_color',\n",
    "    'title_status',\n",
    "    'is_sf_bay_area' # Our engineered binary categorical feature\n",
    "]\n",
    "\n",
    "# Ensure only columns present in the DataFrame are selected for encoding\n",
    "categorical_cols_to_encode = [col for col in categorical_cols_to_encode if col in df.columns]\n",
    "\n",
    "print(f\"\\nCategorical columns selected for One-Hot Encoding: {categorical_cols_to_encode}\")\n",
    "\n",
    "# Perform One-Hot Encoding\n",
    "# drop_first=True helps to avoid multicollinearity (n-1 dummy variables)\n",
    "df_encoded = pd.get_dummies(df, columns=categorical_cols_to_encode, drop_first=True)\n",
    "\n",
    "print(\"\\nShape of DataFrame after One-Hot Encoding:\")\n",
    "print(df_encoded.shape)\n",
    "\n",
    "print(\"\\nFirst 5 rows of the encoded DataFrame (showing some new columns):\")\n",
    "print(df_encoded.head())\n",
    "\n",
    "# Display new columns created by encoding\n",
    "print(\"\\nNew columns created by One-Hot Encoding (first 10 examples):\")\n",
    "new_cols = [col for col in df_encoded.columns if any(cat_col in col for cat_col in categorical_cols_to_encode)]\n",
    "print(new_cols[:10]) # Print first 10 for brevity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df937b33-0310-41e1-a252-4bf1df10bded",
   "metadata": {},
   "source": [
    "#### Step 12: Feature Selection (Final Review) and Model Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a2eb52-6a60-4b4e-a075-bb0dcd02a86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 1. Drop Irrelevant and Redundant Columns\n",
    "# Ensure 'price_log' is explicitly in this list to be dropped from X\n",
    "columns_to_drop_final = [\n",
    "    'id', 'url', 'region_url', 'image_url', 'description', 'VIN', # Identifiers/text\n",
    "    'region', 'posting_date', # These were not used or processed for direct feature input\n",
    "    'county', # Likely dropped due to high NaN, but ensure if it exists\n",
    "    'price', # Original price, we use price_log as target\n",
    "    'price_log', # <--- CRITICAL ADDITION: Ensure this is removed from X!\n",
    "    'year', # 'age' was engineered from this\n",
    "    'odometer', # 'odometer_log' was engineered from this\n",
    "    'cylinders', # 'cylinders_cleaned' was used\n",
    "    'condition', # 'condition_numeric' was used\n",
    "    'model', # Often too high cardinality for simple one-hot encoding without further processing/grouping\n",
    "    'size', # Often many NaNs and too varied. Keep if you think it's useful and cleaned.\n",
    "    'lat', 'long', # Geographic coordinates can be useful but often require more advanced spatial features\n",
    "    'state' # Addressed in previous debug\n",
    "]\n",
    "\n",
    "# Filter columns that actually exist in the DataFrame before dropping\n",
    "columns_to_drop_final = [col for col in columns_to_drop_final if col in df_encoded.columns]\n",
    "\n",
    "print(f\"\\nFinal columns to be dropped from X: {columns_to_drop_final}\")\n",
    "\n",
    "# Create the final feature set X\n",
    "# IMPORTANT: 'errors='ignore' is crucial here in case some columns were already dropped (e.g. by high NaN % earlier)\n",
    "X = df_encoded.drop(columns=columns_to_drop_final, errors='ignore')\n",
    "y = df_encoded['price_log'] # Our target variable\n",
    "\n",
    "print(f\"\\nShape of X (features): {X.shape}\")\n",
    "print(f\"Shape of y (target): {y.shape}\")\n",
    "\n",
    "print(\"\\nFirst 5 rows of X (features) after final drop:\")\n",
    "print(X.head())\n",
    "print(f\"Checking for 'price_log' in X columns: {'price_log' in X.columns}\") # Should be False\n",
    "\n",
    "# 2. Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"\\nShape of X_train: {X_train.shape}\")\n",
    "print(f\"Shape of X_test: {X_test.shape}\")\n",
    "print(f\"Shape of y_train: {y_train.shape}\")\n",
    "print(f\"Shape of y_test: {y_test.shape}\")\n",
    "\n",
    "# Final check: Ensure no missing values remain in X_train/y_train\n",
    "print(\"\\nMissing values in X_train after final preparation:\")\n",
    "print(X_train.isnull().sum().sum())\n",
    "print(\"\\nMissing values in y_train after final preparation:\")\n",
    "print(y_train.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd35458-6766-400e-ab31-cc74be39a642",
   "metadata": {},
   "source": [
    "#### Step 13: Model Training - Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9cbcc4-2b1e-44ad-bb55-4eb9d472b9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import numpy as np # Already imported, but good to ensure\n",
    "\n",
    "# 1. Instantiate the Linear Regression Model\n",
    "linear_model = LinearRegression()\n",
    "\n",
    "# --- INSERT THIS CODE RIGHT BEFORE linear_model.fit(X_train, y_train) ---\n",
    "print(\"\\n--- CRITICAL DEBUG: Final NaN check on X_train and y_train before Model Fit ---\")\n",
    "\n",
    "# Check for NaNs in X_train\n",
    "nan_in_X_train = X_train.isnull().sum()\n",
    "nan_in_X_train = nan_in_X_train[nan_in_X_train > 0]\n",
    "\n",
    "if not nan_in_X_train.empty:\n",
    "    print(\"\\nERROR: NaN values found in X_train columns:\")\n",
    "    print(nan_in_X_train)\n",
    "    print(f\"Total NaN count in X_train: {X_train.isnull().sum().sum()}\")\n",
    "    # Optionally, to see rows with NaNs in X_train:\n",
    "    # print(\"\\nRows in X_train with NaNs (first 5):\")\n",
    "    # print(X_train[X_train.isnull().any(axis=1)].head())\n",
    "    # THIS IS THE SOURCE OF THE PROBLEM! The model cannot train with these.\n",
    "else:\n",
    "    print(\"\\nSUCCESS: No NaN values found in X_train. Proceeding with training.\")\n",
    "\n",
    "# Check for NaNs in y_train\n",
    "nan_in_y_train = y_train.isnull().sum()\n",
    "if nan_in_y_train > 0:\n",
    "    print(f\"\\nERROR: NaN values found in y_train: {nan_in_y_train}\")\n",
    "    # THIS IS THE SOURCE OF THE PROBLEM! The model cannot train with these.\n",
    "else:\n",
    "    print(\"SUCCESS: No NaN values found in y_train.\")\n",
    "\n",
    "print(\"\\n--- End CRITICAL DEBUG ---\")\n",
    "# --- END OF INSERTED CODE ---\n",
    "\n",
    "# linear_model.fit(X_train, y_train) # This line will now be executed after the debug checks\n",
    "# 2. Train the Model\n",
    "# X_train and y_train were created in Step 12\n",
    "print(\"Training Linear Regression model...\")\n",
    "linear_model.fit(X_train, y_train)\n",
    "print(\"Linear Regression model training complete.\")\n",
    "\n",
    "# 3. Make Predictions on the Test Set\n",
    "print(\"Making predictions on the test set...\")\n",
    "y_pred_linear = linear_model.predict(X_test)\n",
    "print(\"Predictions complete.\")\n",
    "\n",
    "# 4. Evaluate the Model's Performance\n",
    "print(\"\\n--- Linear Regression Model Evaluation ---\")\n",
    "\n",
    "# Mean Absolute Error (MAE)\n",
    "mae_linear = mean_absolute_error(y_test, y_pred_linear)\n",
    "print(f\"Mean Absolute Error (MAE): {mae_linear:.4f}\")\n",
    "\n",
    "# Mean Squared Error (MSE)\n",
    "mse_linear = mean_squared_error(y_test, y_pred_linear)\n",
    "print(f\"Mean Squared Error (MSE): {mse_linear:.4f}\")\n",
    "\n",
    "# Root Mean Squared Error (RMSE)\n",
    "rmse_linear = np.sqrt(mse_linear)\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse_linear:.4f}\")\n",
    "\n",
    "# R-squared (R2)\n",
    "r2_linear = r2_score(y_test, y_pred_linear)\n",
    "print(f\"R-squared (R2): {r2_linear:.4f}\")\n",
    "\n",
    "# --- Important: Inverse Transform Predictions ---\n",
    "# Since we trained on price_log, our predictions are in log scale.\n",
    "# To understand the error in actual currency, we need to inverse transform them.\n",
    "y_test_actual = np.expm1(y_test)\n",
    "y_pred_linear_actual = np.expm1(y_pred_linear)\n",
    "\n",
    "mae_linear_actual = mean_absolute_error(y_test_actual, y_pred_linear_actual)\n",
    "rmse_linear_actual = np.sqrt(mean_squared_error(y_test_actual, y_pred_linear_actual))\n",
    "\n",
    "print(f\"\\n--- Linear Regression Model Evaluation (Actual Price Scale) ---\")\n",
    "print(f\"Mean Absolute Error (MAE - Actual Price): ${mae_linear_actual:.2f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE - Actual Price): ${rmse_linear_actual:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642495b0-4704-4bb5-b073-3d881dfe1786",
   "metadata": {},
   "source": [
    "#### Step 14:  Model Training & Evaluation - K-Nearest Neighbors (KNN) Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae454a9d-6a22-4e82-b625-730f15393026",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import numpy as np # Ensure numpy is imported\n",
    "\n",
    "print(\"\\n--- Training K-Nearest Neighbors (KNN) Regressor ---\")\n",
    "\n",
    "# 1. Instantiate the KNN Regressor Model\n",
    "# n_neighbors is a hyperparameter (how many nearest neighbors to consider)\n",
    "# A common starting point is 5 or 7.\n",
    "knn_model = KNeighborsRegressor(n_neighbors=5)\n",
    "\n",
    "# 2. Train the Model\n",
    "# X_train and y_train were created in Step 12\n",
    "print(\"Training KNN Regressor model...\")\n",
    "knn_model.fit(X_train, y_train)\n",
    "print(\"KNN Regressor model training complete.\")\n",
    "\n",
    "# 3. Make Predictions on the Test Set\n",
    "print(\"Making predictions on the test set...\")\n",
    "y_pred_knn = knn_model.predict(X_test)\n",
    "print(\"Predictions complete.\")\n",
    "\n",
    "# 4. Evaluate the Model's Performance\n",
    "print(\"\\n--- KNN Regressor Model Evaluation ---\")\n",
    "\n",
    "# Mean Absolute Error (MAE)\n",
    "mae_knn = mean_absolute_error(y_test, y_pred_knn)\n",
    "print(f\"Mean Absolute Error (MAE): {mae_knn:.4f}\")\n",
    "\n",
    "# Mean Squared Error (MSE)\n",
    "mse_knn = mean_squared_error(y_test, y_pred_knn)\n",
    "print(f\"Mean Squared Error (MSE): {mse_knn:.4f}\")\n",
    "\n",
    "# Root Mean Squared Error (RMSE)\n",
    "rmse_knn = np.sqrt(mse_knn)\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse_knn:.4f}\")\n",
    "\n",
    "# R-squared (R2)\n",
    "r2_knn = r2_score(y_test, y_pred_knn)\n",
    "print(f\"R-squared (R2): {r2_knn:.4f}\")\n",
    "\n",
    "# --- Important: Inverse Transform Predictions for Actual Price Scale ---\n",
    "y_test_actual = np.expm1(y_test)\n",
    "y_pred_knn_actual = np.expm1(y_pred_knn)\n",
    "\n",
    "mae_knn_actual = mean_absolute_error(y_test_actual, y_pred_knn_actual)\n",
    "rmse_knn_actual = np.sqrt(mean_squared_error(y_test_actual, y_pred_knn_actual))\n",
    "\n",
    "print(f\"\\n--- KNN Regressor Model Evaluation (Actual Price Scale) ---\")\n",
    "print(f\"Mean Absolute Error (MAE - Actual Price): ${mae_knn_actual:.2f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE - Actual Price): ${rmse_knn_actual:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a69525-b851-4880-9889-736465e4f199",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3d8d82-18ac-4fea-8021-86982376cef8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
